{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install intrinio-sdk #installing intrinio - sdk"
      ],
      "metadata": {
        "id": "PK7lCCpt648S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import intrinio_sdk as intrinio\n",
        "intrinio.ApiClient().set_api_key('Your_API-KEY')#setting up the API key\n",
        "#NOTE : Replace Your_API-KEY with you actual API key\n",
        "identifier = 'AAPL' # Setting up the identifier\n",
        "source = 'nasdaq_basic' # Setting up the source\n",
        "response = intrinio.SecurityApi().get_security_realtime_price(identifier, source=source) # Extracting the data\n",
        "print(response)"
      ],
      "metadata": {
        "id": "s5dHK3SC60RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import intrinio_sdk\n",
        "import time\n",
        "\n",
        "\n",
        "class QLearningTrader:\n",
        "    def __init__(self, num_actions, num_features, learning_rate, discount_factor, exploration_prob):\n",
        "        self.num_actions = num_actions\n",
        "        self.num_features = num_features\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_prob = exploration_prob\n",
        "\n",
        "\n",
        "        # Initialize Q-table with zeros\n",
        "        self.q_table = np.zeros((num_actions, num_features))\n",
        "\n",
        "\n",
        "        # Initialize state and action\n",
        "        self.current_state = None\n",
        "        self.current_action = None\n",
        "\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        # Exploration-exploitation trade-off\n",
        "        if np.random.uniform(0, 1) < self.exploration_prob:\n",
        "            return np.random.choice(self.num_actions)  # Explore\n",
        "        else:\n",
        "            feature_index = np.argmax(state)\n",
        "            return np.argmax(self.q_table[:, feature_index])  # Exploit\n",
        "\n",
        "\n",
        "    def take_action(self, action, reward):\n",
        "        # Update Q-table based on the observed reward\n",
        "        if self.current_action is not None:\n",
        "            feature_index = np.argmax(self.current_state)\n",
        "            current_q_value = self.q_table[self.current_action, feature_index]\n",
        "            new_q_value = (1 - self.learning_rate) * current_q_value + \\\n",
        "                           self.learning_rate * (reward + self.discount_factor * np.max(self.q_table[:, feature_index]))\n",
        "            self.q_table[self.current_action, feature_index] = new_q_value\n",
        "\n",
        "\n",
        "        # Update current state and action\n",
        "        self.current_state = None\n",
        "        self.current_action = action\n",
        "\n",
        "\n",
        "    def observe_real_time_data(self, identifier):\n",
        "        # Fetch real-time data\n",
        "        real_time_data = fetch_real_time_data(identifier)\n",
        "\n",
        "\n",
        "        # Extract features from real-time data\n",
        "        self.current_state = np.array([real_time_data['open'], real_time_data['high'],\n",
        "                                       real_time_data['low'], real_time_data['close'],\n",
        "                                       real_time_data['volume']])\n",
        "\n",
        "\n",
        "    def observe_next_state(self, identifier):\n",
        "        # Update the current state with the observed next state\n",
        "        self.current_state = fetch_real_time_data(identifier)\n",
        "\n",
        "\n",
        "def fetch_real_time_data(identifier):\n",
        "    source = 'nasdaq_basic'\n",
        "    response = intrinio.SecurityApi().get_security_realtime_price(identifier, source=source)\n",
        "\n",
        "\n",
        "    return {\n",
        "        'open': response.open_price,\n",
        "        'high': response.high_price,\n",
        "        'low': response.low_price,\n",
        "        'close': response.last_price,\n",
        "        'volume': response.last_size\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_reward(action, current_close, next_close):\n",
        "    if action == 0:  # Buy\n",
        "        return 1.0 if next_close > current_close else -1.0\n",
        "    elif action == 1:  # Sell\n",
        "        return 1.0 if next_close < current_close else -1.0\n",
        "    else:  # Hold\n",
        "        return 1.0 if next_close > current_close else -1.0 if next_close < current_close else 0.0\n",
        "\n",
        "\n",
        "def calculate_profit_loss(initial_balance, suggested_action, current_close, next_close, quantity):\n",
        "    if suggested_action == \"Buy\":\n",
        "        return (next_close - current_close) * quantity\n",
        "    elif suggested_action == \"Sell\":\n",
        "        return (current_close - next_close) * quantity\n",
        "    else:  # Hold\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def calculate_final_profit(identifier, initial_balance, quantity, num_iterations, learning_rate, discount_factor, exploration_prob):\n",
        "    num_actions = 3\n",
        "    num_features = 5\n",
        "    q_trader = QLearningTrader(num_actions, num_features, learning_rate, discount_factor, exploration_prob)\n",
        "\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        q_trader.observe_real_time_data(identifier)\n",
        "\n",
        "\n",
        "        action = q_trader.choose_action(q_trader.current_state)\n",
        "        current_close = q_trader.current_state[3]\n",
        "\n",
        "\n",
        "        time.sleep(1)  # Introduce a delay before fetching the next real-time data\n",
        "\n",
        "\n",
        "        q_trader.observe_next_state(identifier)\n",
        "        next_close = q_trader.current_state['close']\n",
        "\n",
        "\n",
        "        reward = calculate_reward(action, current_close, next_close)\n",
        "        q_trader.take_action(action, reward)\n",
        "\n",
        "\n",
        "    # Fetch real-time data just after the last iteration\n",
        "    final_real_time_data = fetch_real_time_data(identifier)\n",
        "\n",
        "\n",
        "    # Get the final suggested action based on the last state in the Q-table\n",
        "    final_suggested_action = [\"Buy\", \"Sell\", \"Hold\"][np.argmax(q_trader.q_table[:, np.argmax(q_trader.current_state)])]\n",
        "\n",
        "\n",
        "    # Calculate profit based on the final suggested action\n",
        "    final_profit = calculate_profit_loss(initial_balance, final_suggested_action, current_close, final_real_time_data['close'], quantity)\n",
        "\n",
        "\n",
        "    print(f\"Final Suggested Action: {final_suggested_action}, Final Profit: {final_profit}\")"
      ],
      "metadata": {
        "id": "P75hnEqZ7Aon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "security_identifier = 'AAPL'\n",
        "calculate_final_profit(security_identifier, initial_balance=100, quantity=10, num_iterations=180, learning_rate=0.1, discount_factor=0.9, exploration_prob=0.2)"
      ],
      "metadata": {
        "id": "6yN2ntDl7IY5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}